# Ollama Server Host (optional, defaults to http://localhost:11434)
# OLLAMA_HOST=http://localhost:11434

# Note: No API key required for local Llama models via Ollama
# Install Ollama from: https://ollama.ai
# Run: ollama serve
# Pull a model: ollama pull llama3.2
